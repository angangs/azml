{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa768a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adal import AuthenticationContext\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline, StepSequence\n",
    "from azureml.pipeline.core.schedule import Schedule\n",
    "from azureml.pipeline.steps import PythonScriptStep, ParallelRunStep, ParallelRunConfig\n",
    "import os\n",
    "import requests\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167df37",
   "metadata": {},
   "source": [
    "# Workspace Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08177b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_key = \"\" # Retrieved by the user from Storage Account -> Access Keys -> Show Keys -> key1 (Key)\n",
    "blob_datastore_name = \"\" # Given by the user, the name of Datastore to be registered \n",
    "CONTAINER_NAME = \"\" # Given by the user, the Blob container name in the desired Storage account\n",
    "dataset_type = \"\" # Given by the user,  either 'Tabular' or 'File'\n",
    "local_path = os.getcwd() + \"your_desired_local_path\" # Given by the user, The local path to store the config_file if create_ws is True in setup_workspace()\n",
    "LOCATION = \"\" # Given by the user, the desired location of Resource Group and AzureML workspace\n",
    "RESOURCE_GROUP_NAME = \"\" # Given by the user, the Resource Group Name\n",
    "\n",
    "# When using register_datastore(), STORAGE_ACCOUNT_NAME is required given by the user. \n",
    "# When using setup_workspace(), if you want to create the AzureML workspace, you can either provide your own Storage Account\n",
    "# and associate it with the AzureML resource by setting use_my_storage = True and create_ws = True. \n",
    "# Note that you may need to add 'Contributor' and 'Storage Blob Data Contributor' roles to the AzureML workspace under Storage Account's Access Control\n",
    "# If you don't set use_my_storage to True, AzureML workspace will create a default Storage Account.\n",
    "# Otherwise, if you just retrieve the Workspace object, the STORAGE_ACCOUNT_NAME is not necessary\n",
    "STORAGE_ACCOUNT_NAME = \"\"\n",
    "\n",
    "subscription_id = \"\" # Given by the user\n",
    "ws_name = \"\" # Given by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115a254",
   "metadata": {},
   "source": [
    "# Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4914913",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOBNAME_train = \"Training_Data.csv\" # Given by the user, the path to the file, including any subdirectories and filename\n",
    "BLOBNAME_test = \"Test_Data.csv\" # # Given by the user, the path to the file, including any subdirectories and filename\n",
    "compute_name = \"computeMarshall\" # Given by the user, the name of the ComputeTarget\n",
    "compute_type = \"cluster\" # Given by the user, the type of the ComputeTarget, either Compute Instance ('instance') or Compute Cluster ('cluster')\n",
    "\n",
    "# Given by the user, the encoding of TabularDataset object (eg: utf-8, iso88591 etc.)\n",
    "# Supported encodings are 'utf8', 'iso88591', 'latin1', 'ascii', 'utf16', 'utf32', 'utf8bom' and 'windows1252'\n",
    "encoding = 'iso88591' \n",
    "\n",
    "env_name = \"TestMarshallEnv\" # Given by the user, the Name of the Environment to be used in the pipeline runs\n",
    "features = \"Features\"\n",
    "model_name = \"Model\"\n",
    "pipeline_name = \"train-pipeline-test\"\n",
    "pip_packages = ['pandas', 'scikit-learn', 'azureml-sdk', 'nltk', 'xgboost', 'azureml-dataset-runtime[fuse,pandas]'] # Given by the user, the list of pip packages for the custom Environment\n",
    "prep_inference_data = 'test_dtm.csv'\n",
    "prep_training_data = 'train_dtm.csv'\n",
    "train_dataset_name = 'train_ds' # Given by the user, the desired name of the registered Dataset\n",
    "test_dataset_name = 'test_ds' # Given by the user, the desired name of the registered Dataset\n",
    "vm_size = 'STANDARD_D1_V2' # Given by the user, the size of virtual machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a6ebb",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923f021",
   "metadata": {},
   "source": [
    "## Setup ML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a517c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws = setup_workspace(local_path, LOCATION, RESOURCE_GROUP_NAME, \n",
    "                     STORAGE_ACCOUNT_NAME, subscription_id, ws_name)\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4297b3e",
   "metadata": {},
   "source": [
    "## Register Datastore (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_datastore(account_key, blob_datastore_name, CONTAINER_NAME,\n",
    "                   STORAGE_ACCOUNT_NAME, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bc2ac",
   "metadata": {},
   "source": [
    "## Register Datasets (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_dataset(blob_datastore_name, BLOBNAME_train, BLOBNAME_test, dataset_type,\n",
    "                 encoding, test_dataset_name, train_dataset_name, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40056d06",
   "metadata": {},
   "source": [
    "## Retrieve datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = retrieve_dataset(test_dataset_name, train_dataset_name, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028b5ad",
   "metadata": {},
   "source": [
    "## Prepare Compute Target and Pipeline configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff57116",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target, run_config, environment = prepare_pipeline(compute_name, compute_type, env_name, \n",
    "                                                           vm_size, ws, pip_packages=pip_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686d8a5",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05521cd",
   "metadata": {},
   "source": [
    "#### prep_data is output of the 1st step and input to the 2nd step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = PipelineData('prep_train_data', datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a81bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep_step = PythonScriptStep(\n",
    "    script_name=\"data_prep.py\",\n",
    "    name = '01 Data Preprocessing',\n",
    "    allow_reuse=False,\n",
    "    arguments=[\"--datafolder\", prep_data,\n",
    "               \"--input\", train_dataset_name,\n",
    "               \"--output_name\", prep_training_data,\n",
    "               \"--process\", \"training\"],\n",
    "    inputs=[train_ds.as_named_input(train_dataset_name)],\n",
    "    outputs=[prep_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/src/AzureML_NonSpark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step1 = PythonScriptStep(\n",
    "    script_name=\"train.py\",\n",
    "    name = '02 Training',\n",
    "    allow_reuse=False,\n",
    "    arguments=[\"--datafolder\", prep_data,\n",
    "               \"--input_filename\", prep_training_data],\n",
    "    inputs=[prep_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/src/AzureML_NonSpark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step2 = PythonScriptStep(\n",
    "    script_name=\"train.py\",\n",
    "    name = '02 Training',\n",
    "    allow_reuse=False,\n",
    "    arguments=[\"--datafolder\", prep_data,\n",
    "               \"--input_filename\", prep_training_data],\n",
    "    inputs=[prep_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/src/AzureML_NonSpark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step = PythonScriptStep(\n",
    "    script_name=\"log.py\",\n",
    "    name = 'Logging Run Status',\n",
    "    arguments=[\"--process\", \"Training\",\n",
    "               \"--outputfolder\", \"outputs\"],\n",
    "    allow_reuse=False,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/src/AzureML_NonSpark' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name=\"train-pipeline-december-no-spark\")\n",
    "step_sequence = StepSequence(steps=[dataprep_step, train_step, log_step])\n",
    "train_pipeline = Pipeline(workspace=ws, steps=step_sequence)\n",
    "train_pipeline_run = experiment.submit(train_pipeline, continue_on_step_failure=True)\n",
    "train_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78ff8e",
   "metadata": {},
   "source": [
    "## Publish Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = train_pipeline.publish(name=\"Training - December\",\n",
    "                                            description=\"Model training pipeline\",\n",
    "                                            version=\"1.0\",\n",
    "                                            continue_on_step_failure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2139b",
   "metadata": {},
   "source": [
    "## Get published pipeline and enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pipeline by using its ID from Azure Machine Learning studio\n",
    "pipeline_id = \"ffc7afa7-10d6-4de2-87ef-a438c9b42c49\"\n",
    "p = PublishedPipeline.get(ws, id=pipeline_id)\n",
    "# p.enable()\n",
    "# p.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b01610",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdf2d5",
   "metadata": {},
   "source": [
    "## Reactive schedule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4163c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipelines = PublishedPipeline.list(ws)\n",
    "for published_pipeline in published_pipelines:\n",
    "    pipeline_id = published_pipeline.id\n",
    "    print(f\"{published_pipeline.name},'{published_pipeline.id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39786bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = Datastore(workspace=ws, name=\"test_datastore\")\n",
    "experiment_name = \"train-pipeline-december-no-spark\"\n",
    "path_on_datastore = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule = Schedule.create(ws, name=\"MyReactiveSchedule\", description=\"Based on input file change.\",\n",
    "                                    pipeline_id=pipeline_id, experiment_name=experiment_name, datastore=datastore, \n",
    "                                    continue_on_step_failure=True, path_on_datastore=path_on_datastore,\n",
    "                                    polling_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c378383",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ed2bd",
   "metadata": {},
   "source": [
    "### Note you cannot disable a published pipeline that has an active Schedule. You must first disable the schedule and then the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ddf54",
   "metadata": {},
   "source": [
    "#### List all schedules of workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Schedule.list(ws)[0].disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7722a98",
   "metadata": {},
   "source": [
    "# Run a published pipeline through REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "auth_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(p.endpoint,\n",
    "                         json={\"ExperimentName\": \"train-pipeline-december-no-spark\"},\n",
    "                         headers=auth_header\n",
    "                         )\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2e06a",
   "metadata": {},
   "source": [
    "### Retrieve Pipeline run status through REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51babf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "runId = \"4021514c-a44f-4ce9-a50b-68bc5e4765a0\"\n",
    "experiment_name = \"train-pipeline-december-no-spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = f'https://{LOCATION}.experiments.azureml.net/history/v1.0/subscriptions/{subscription_id}/resourceGroups/{RESOURCE_GROUP_NAME}/providers/Microsoft.MachineLearningServices/workspaces/{ws_name}/experiments/{experiment_name}/runs/{runId}/details'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_get = requests.get(get_endpoint,\n",
    "                         headers=auth_header\n",
    "                         )\n",
    "\n",
    "\n",
    "response_get.json()['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760dada",
   "metadata": {},
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8accbf",
   "metadata": {},
   "source": [
    "#### prep_inf_data is output of the 1st step and input of the 2nd step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6151659",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_inf_data = PipelineData('prep_inf_data', datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep_step = PythonScriptStep(\n",
    "    script_name=\"data_prep.py\",\n",
    "    name = '01 Data Preprocessing',\n",
    "    arguments=[\"--datafolder\", prep_inf_data,\n",
    "               \"--input\", test_dataset_name,\n",
    "               \"--output_name\", prep_inference_data,\n",
    "               \"--process\", \"inference\"],\n",
    "    inputs=[test_ds.as_named_input(test_dataset_name)],\n",
    "    outputs=[prep_inf_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_NonSpark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b12f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_step = PythonScriptStep(\n",
    "    script_name=\"inference.py\",\n",
    "    name = '02 Predict',\n",
    "    arguments=[\"--model_name\", model_name,\n",
    "               \"--datafolder\", prep_inf_data,\n",
    "               \"--features\", features,\n",
    "               \"--input_filename\", prep_inference_data],\n",
    "    inputs=[prep_inf_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_NonSpark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a50ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step = PythonScriptStep(\n",
    "    script_name=\"log.py\",\n",
    "    name = 'Logging Run Status',\n",
    "    arguments=[\"--process\", \"Inference\",\n",
    "               \"--outputfolder\", \"outputs\"],\n",
    "    allow_reuse=False,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_NonSpark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='inference-pipeline-test')\n",
    "step_sequence = StepSequence(steps=[dataprep_step, inference_step, log_step])\n",
    "inference_pipeline = Pipeline(workspace=ws, steps=step_sequence)\n",
    "inference_pipeline_run = experiment.submit(inference_pipeline, continue_on_step_failure=True)\n",
    "inference_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = inference_pipeline.publish(name=\"Inference\",\n",
    "                                            description=\"Model inference pipeline\",\n",
    "                                            version=\"1.0\",\n",
    "                                            continue_on_step_failure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
