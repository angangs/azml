{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa768a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adal import AuthenticationContext\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core import Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.compute import SynapseCompute\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline, StepSequence\n",
    "from azureml.pipeline.core.schedule import Schedule\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep, SynapseSparkStep\n",
    "import os\n",
    "import requests\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03195326",
   "metadata": {},
   "source": [
    "# Synapse Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_compute_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167df37",
   "metadata": {},
   "source": [
    "# Workspace Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08177b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_key = \"\"\n",
    "blob_datastore_name = \"\"\n",
    "CONTAINER_NAME = \"\"\n",
    "local_path = os.getcwd() + \"your_desired_local_path\"\n",
    "LOCATION = \"\"\n",
    "RESOURCE_GROUP_NAME = \"\"\n",
    "STORAGE_ACCOUNT_NAME = \"\"\n",
    "subscription_id = \"\"\n",
    "ws_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115a254",
   "metadata": {},
   "source": [
    "# Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4914913",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOBNAME_train = \"\"\n",
    "BLOBNAME_test = \"\"\n",
    "compute_name = \"\"\n",
    "compute_type = \"\"\n",
    "encoding = \"iso88591\"\n",
    "env_name = \"\"\n",
    "model_name = \"\"\n",
    "pipeline_name = \"\"\n",
    "pip_packages = ['pandas', 'scikit-learn', 'azureml-sdk', 'nltk', 'xgboost']\n",
    "prep_inference_data = \"\"\n",
    "prep_training_data = \"\"\n",
    "train_dataset_name = \"\"\n",
    "test_dataset_name = \"\"\n",
    "vm_size = \"STANDARD_D1_V2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a6ebb",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923f021",
   "metadata": {},
   "source": [
    "## Setup ML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a517c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws = setup_workspace(local_path, LOCATION, RESOURCE_GROUP_NAME, \n",
    "                     STORAGE_ACCOUNT_NAME, subscription_id, ws_name)\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbe660",
   "metadata": {},
   "source": [
    "## Register new datastore (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_datastore(account_key, blob_datastore_name, CONTAINER_NAME,\n",
    "                       STORAGE_ACCOUNT_NAME, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78dbdd",
   "metadata": {},
   "source": [
    "## Register train and test datasets (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561016d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_dataset(blob_datastore_name, BLOBNAME_train, BLOBNAME_test, dataset_type,\n",
    "                     encoding, test_dataset_name, train_dataset_name, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40056d06",
   "metadata": {},
   "source": [
    "## Retrieve datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = retrieve_dataset(test_dataset_name, train_dataset_name, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028b5ad",
   "metadata": {},
   "source": [
    "## Prepare Compute Target and Pipeline configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff57116",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target, run_config, environment = prepare_pipeline(compute_name, compute_type, env_name, \n",
    "                                              vm_size, ws, pip_packages=pip_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686d8a5",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871e2e4",
   "metadata": {},
   "source": [
    "#### Default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3d45",
   "metadata": {},
   "source": [
    "#### Output logs file to datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = OutputFileDatasetConfig(destination=(Datastore(ws, \"synapse_datastore\"), 'runLogs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35ebc7",
   "metadata": {},
   "source": [
    "#### Input data and output data for SynapseSparkStep 1 and input data for PythonScriptStep 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111d896",
   "metadata": {},
   "source": [
    "#### step1_input1 is a registered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe82fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_input1 = train_ds.as_named_input('TrainingDataSynapse')\n",
    "step1_output = HDFSOutputDatasetConfig(destination=(def_blob_store,\"train\")).register_on_complete(name=\"TrainDTM\")\n",
    "\n",
    "step2_input = step1_output.as_input(\"TrainDTM\").as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep_step = SynapseSparkStep(name = '01 Data Preprocessing',\n",
    "                          file = 'spark_data_prep.py',\n",
    "                          source_directory=os.getcwd() + '/AzureML_SparkPreprocessing',\n",
    "                          inputs=[step1_input1],\n",
    "                          outputs=[step1_output],\n",
    "                          allow_reuse=False,\n",
    "                          arguments=[\"--input\", step1_input1,\n",
    "                                     \"--output_dir\", step1_output,\n",
    "                                     \"--process\", \"Training\"],\n",
    "                          compute_target = synapse_compute_name,\n",
    "                          driver_memory = \"7g\",\n",
    "                          driver_cores = 4,\n",
    "                          executor_memory = \"7g\",\n",
    "                          executor_cores = 4,\n",
    "                          num_executors = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "    script_name=\"train.py\",\n",
    "    name = '02 Training',\n",
    "    allow_reuse=False,\n",
    "    arguments=[step2_input],\n",
    "    inputs=[step2_input],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_SparkPreprocessing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step = PythonScriptStep(\n",
    "    script_name=\"log.py\",\n",
    "    name = '03 Logging Run Status',\n",
    "    arguments=[\"--process\", \"Training\",\n",
    "               \"--outputfolder\", log_data],\n",
    "    allow_reuse=False,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_SparkPreprocessing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name=pipeline_name)\n",
    "step_sequence = StepSequence(steps=[dataprep_step, train_step, log_step])\n",
    "train_pipeline = Pipeline(workspace=ws, steps=step_sequence)\n",
    "train_pipeline_run = experiment.submit(train_pipeline, continue_on_step_failure=True)\n",
    "train_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78ff8e",
   "metadata": {},
   "source": [
    "## Publish Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = train_pipeline.publish(name=\"Training Pipeline\",\n",
    "                                            description=\"Training pipeline\",\n",
    "                                            version=\"1.0\",\n",
    "                                            continue_on_step_failure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38784f94",
   "metadata": {},
   "source": [
    "# Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc194b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_id = \"\"\n",
    "experiment_name = \"\"\n",
    "datastore = Datastore(workspace=ws, name=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fb21e",
   "metadata": {},
   "source": [
    "## Run pipeline with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PublishedPipeline.get(ws, id=pipeline_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "auth_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(p.endpoint,\n",
    "                         json={\"ExperimentName\": experiment_name},\n",
    "                         headers=auth_header\n",
    "                         )\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515babe6",
   "metadata": {},
   "source": [
    "## Create change based event (on Blob change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_datastore = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f497b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule = Schedule.create(ws, name=\"MyReactiveSchedule\", description=\"Based on input file change.\",\n",
    "                                    pipeline_id=pipeline_id, experiment_name=experiment_name, datastore=datastore, \n",
    "                                    continue_on_step_failure=True, path_on_datastore=path_on_datastore,\n",
    "                                    polling_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b20b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92852adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0201da4",
   "metadata": {},
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6074d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bb211",
   "metadata": {},
   "source": [
    "#### Output data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = OutputFileDatasetConfig(destination=(Datastore(ws, \"synapse_datastore\"), 'runLogs'))\n",
    "prediction_data = OutputFileDatasetConfig(destination=(Datastore(ws, \"synapse_datastore\"), 'Predictions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69636da2",
   "metadata": {},
   "source": [
    "#### Input/ouput datasets during pipeline run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a265595",
   "metadata": {},
   "source": [
    "#### train_dtm_input is a Registered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a44871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtm_input = Dataset.get_by_name(ws, \"TrainDTM\").as_named_input('TrainDTM').as_hdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dce2e5",
   "metadata": {},
   "source": [
    "#### Input data and output data for SynapseSparkStep 1 and input data for PythonScriptStep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f26a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_test_input1 = test_ds.as_named_input('InferenceDataSynapse')\n",
    "step1_test_output = HDFSOutputDatasetConfig(destination=(def_blob_store,\"inference\"))\n",
    "\n",
    "step2_test_input = step1_test_output.as_input(\"InferenceDTM\").as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep_step = SynapseSparkStep(name = '01 Data Preprocessing',\n",
    "                          file = 'spark_data_prep.py',\n",
    "                          source_directory=os.getcwd() + '/AzureML_SparkPreprocessing', \n",
    "                          inputs=[train_dtm_input, step1_test_input1],\n",
    "                          outputs=[step1_test_output],\n",
    "                          allow_reuse=False,\n",
    "                          arguments=[\"--train_input\", train_dtm_input,\n",
    "                                     \"--input\", step1_test_input1,\n",
    "                                     \"--output_dir\", step1_test_output,\n",
    "                                     \"--process\", \"Inference\"],\n",
    "                          compute_target = synapse_compute_name,\n",
    "                          driver_memory = \"7g\",\n",
    "                          driver_cores = 4,\n",
    "                          executor_memory = \"7g\",\n",
    "                          executor_cores = 4,\n",
    "                          num_executors = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_step = PythonScriptStep(\n",
    "    script_name=\"inference.py\",\n",
    "    name = '02 Predict',\n",
    "    arguments=[\"--model_name\", model_name,\n",
    "               \"--outputfolder\", prediction_data],\n",
    "    inputs=[step2_test_input],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_SparkPreprocessing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9219a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step = PythonScriptStep(\n",
    "    script_name=\"log.py\",\n",
    "    name = '03 Logging Run Status',\n",
    "    arguments=[\"--process\", \"Inference\",\n",
    "               \"--outputfolder\", log_data],\n",
    "    allow_reuse=False,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.getcwd() + '/AzureML_SparkPreprocessing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='spark-inference-pipeline-test')\n",
    "step_sequence = StepSequence(steps=[dataprep_step, predict_step, log_step])\n",
    "inference_pipeline = Pipeline(workspace=ws, steps=step_sequence)\n",
    "inference_pipeline_run = experiment.submit(inference_pipeline, continue_on_step_failure=True)\n",
    "inference_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_published_pipeline = inference_pipeline.publish(name=\"Inference Pipeline\",\n",
    "                                            description=\"Inference pipeline\",\n",
    "                                            version=\"1.0\",\n",
    "                                            continue_on_step_failure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d3f37",
   "metadata": {},
   "source": [
    "## Create change based event (on Blob change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f76862",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_datastore = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_schedule = Schedule.create(ws, name=\"MyReactiveSchedule\", description=\"Based on input file change.\",\n",
    "                                    pipeline_id=pipeline_id, experiment_name=experiment_name, datastore=datastore, \n",
    "                                    continue_on_step_failure=True, path_on_datastore=path_on_datastore,\n",
    "                                    polling_interval=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
